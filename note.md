# tensorflow- #
学习使用tensorflow，（2018年3月5日 10:39:54）  
```
换行方法：每行末尾加两个空格
```
## 神经网络优化算法 ##

### 1.梯度下降算法 ###
> 梯度算法是基于全局的损失和进行的参数跟新，每次更新需要对全部数据进行训练，更新参数后仍然需要对全部训练数据进行训练。（可以达到局部最优，不能达到全局最优）
### 2.随机梯度下降算法 ###
> 随机梯度下降算法是：在每一轮的迭代中，随机优化某一条数据上的损失函数（可能无法达到局部最优）
### 3.学习率问题 ###
> 学习率决定了参数每次更新的幅度，如果幅度过大，可能导致参数在极优值两侧来回移动，如果过小，则收敛速度慢
####  指数衰减法： ####
   > 让模型在训练的前期快速接近较优解，又可以保证在模型训练的后期不会有太大的波动，从而更加接近局部最优。
   
    1. 衰减学习率 = 学习率*衰减率^（全局迭代步长/衰减步长）
        1. 学习率： 事先设定的初始学习率
        2. 衰减率： 衰减系数（每一个衰减步长下，学习率衰减程度。）
        3. 全局迭代步长： 已经训练数据的轮数
        4. 衰减步长：设定的每执行多少轮数据训练，进行学习率衰减   
    2. tf.train.exponential_decay(初始学习率，全局迭代步长，衰减步长，衰减率，衰减方式) ： tensorflow提供的指数衰减函数
        1. 衰减方式：staircase 布尔类型
            1. False：默认值，此时学习率的衰减成曲线衰减，（全局迭代步长/衰减步长）成线性减小
            2. true： 此时学习率成阶梯状衰减,（全局迭代步长/衰减步长）被取整，程散列减小
### 4.过拟合问题 ###
> 当一个模型过于复杂之后，它可能很好的模拟每一个训练数据中随机噪音部分而冲淡了学习训练数据中通用的行为。
> 模型在训练过程中，为了充分模拟训练数据的行为，保有了很多特征参数，又因为训练数据不足以确定所有参数合适的值，使得在实际应用中与预测
> 偏差较大，出现过拟合。
> 训练数据集样本单一，也是导致过拟合问题的一个原因，还有训练噪声太大，也能导致过拟合问题。（来自《机器学习实践应用》李博）

？？？？特征参数是怎么生成的，特征参数的数量怎么控制的

> 解决过拟合有两种方法，一是主动减少特征参数的个数，但是选择保留特征参数很复杂，不能保证减去的都是无用的特征参数，这会造成模型效果不好
> 在一个就是正则化，它会保留所有特征参数  
> 解决过拟合三个注意：（来自《机器学习实践应用》李博

 1. 训练和建立模型的时候，一定要从简单的模型开始训练
 2. 训练数据，尽可能的覆盖全部数据类型，数据需要清洗后才能进行算法训练
 3. 通过添加惩罚函数预防过拟合 

#### 正则化 ####
[正则化详细解释](https://www.zhihu.com/question/20700829)
> 在损失函数中加入刻画模型复杂度的指标（模型复杂度：神经网络模型中所有特征参数综合的表现，可以理解为之和的大小)   
> 通过控制特征参数的大小，即是特征参数越小，其对输出结果的影响越小，如果无限接近于零，那么这个特征参数就不再影响输出结果，
> *** 
> 函数：J(θ)+λR(W)  
> J(θ): 代表损失函数  
> λ ：  代表模型复杂损失在总损失的比例，即是每次优化，模型复杂度被优化的比例  
> R(W)：模型的复杂程度，w为神经网络上的所有参数，包括权重w和偏置项b   
> L1正则化： R(w)=‖W‖₁ = Σ｜W｜  
> L2正则换： R(w)=‖W‖² = Σ｜W²｜  

### 特征参数健壮性，降低噪声干扰 ###
#### 滑动平均模型 ####

[滑动平均模型详细说明](http://blog.csdn.net/IAMoldpan/article/details/78208897?locationNum=11&fps=1)

#### 前馈神经网络 ####
> 多层前馈网络由一个输入层、一个输出层和若干个隐含层组成 . 同层之间的节点没有联接 ,相邻两层之间的节点两两相连 ,前一层节点的输出即为后一层节点的输入 . 它的基本运行机制是: 给网络赋给初始权值和阀值 ,按照给定的样本前向计算网络输出 ,根据实际输出与期望输出之间的误差 ,反向修改网络权值和阀值 ,如此反复训练使误差达到最小 ,用得到的网络模拟样本的输入输出关系



## 笔记 ##
#### 泛化能力 ####
前馈神经网络的泛化能力 ( generalization capability )是指对于同一样本集中的非训练样本 ,网络仍能给出正确的输入— 输出关系的能力 [4] 

#### 单隐层于多隐层对比 ####
 由于对非线性优化问题 ,目前从理论上保证收敛到全局最优解的算法不多 ,而且实现起来十分困难 ,这使得网络在学习和训练过程中容易陷入局部极小点 ,而网络越复杂 ,所要求解的非线性优化问题就越复杂 ,网络陷入局部极小点的可能性也就越大 ,这是造成复杂网络不如简单网络泛化能力强的主要原因 .
#### 极限学习机原理 ####
极限学习机是基于已证明的插值定理和普通极限定理两个定理提出的。这两个定理证明了当单隐层前馈神经网络的隐层映射函数满足无限可微时，其学习能力与输入权重和阈值等参数选取没有相关性，仅仅只跟当前的网络结构有关。当选取合适的网络结构的时候，该神经网络就能无误差地拟合任何连续函数。现在大多数的极限学习机模型都釆用简单的随机方式来获得输入权重和阈值，独立于训练数据，避免对训练数据产生过度拟合的问题。
#### 为什么在隐藏层到输出层间只需要权重，不需要偏置项 ####

偏置项是在隐藏层出现的，每一个隐藏层对应一个偏置项位置，g(wx+b),激活函数整体相当于一个隐藏层空间，权重取决于上一层节点到当前节点的权重，偏置项同样如此，即使，权重在连线上，偏置项在隐藏层上。




## Extreme learning machine：theory and applications ##
### theory ###
> 超限学习机（ELM）是一种单隐层前馈神经网络，它克服了传统的人工神经网络上述两个缺点。首先，ELM 中输入层与隐层之间的连接权和偏置都是随机产生的，大幅减少了网络中需要训练的参数，在一定程度上缩短了网络的学习时间；其次，ELM 的学习方法不需要反复迭代，并且保证得到的解唯一。第一点是因为 Tamura 和 Huang 等人在文献中证明了：当训练样本的个数小于或等于隐层节点个数的时候，对于随机设置输入层到隐层的连接权和偏置的单隐层前馈神经网络模型，仅调节隐层到输出层的连接权便能够使网络拥有完全区分训练数据的能力。第二点的原因主要有三个：1）由于在超限学习机模型中输入层到隐层的连接权和偏置已经确定了，这样便可以直接求出所有输入样本在模型隐层中的激活值；2）通过进一步化简目标函数，将求解模型中隐层与输出层之间连接权的问题转化为求最小二乘的问题；3）将最小二乘的求解问题转化为求Moore-Penrose 广义逆[46]的过程，能够得到唯一的范数最小解。